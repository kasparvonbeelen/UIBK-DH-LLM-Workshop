{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "Tq0pWVJXoo10"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOXipQJ/rRpoTZqf12JuAAh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kasparvonbeelen/UIBK-DH-LLM-Workshop/blob/dev/LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using open-source LLMs for analysing humanities data"
      ],
      "metadata": {
        "id": "S-2ldxb0iUxG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whereas the examples in the previous notebook focussed on model predictions, we now explore applications for generative AI for processing and analysing historical newspapers.\n",
        "\n",
        "Instead of investigating how the model works, we focus on what we can do with the outputs.\n",
        "\n",
        "Major hurdles to working with LLMs are cost and/or infrastructure. Opposed to GPT-2 or BERT, running LLMs locally can be difficult, and using commercial APIs can be expensive.\n",
        "\n",
        "## Why Open-source?\n",
        "\n",
        "- **Privacy:**: You might not want to share your data (and ideas) with companies such as OpenAI;\n",
        "- **Cost:** Making abstraction of the caveat above, using open-source models might reduce costs if you want to apply for example a prompt to 10k newspaper articles;\n",
        "- **Transparency:** Be mindful that there are different gradations of openness and transparency. Even when you can access the model weights, you might remain in the dark about training data and other factors);\n",
        "- **Flexibility:** Even though some providers allow you to train or fine-tune closed models on your data (ties in with privacy), open-source models still give you more freedom and wiggle room to build new models and applications.\n",
        "\n",
        "## Why Large Language Models\n",
        "\n",
        "Text analysis in digital humanities is often described as 'distant reading', or \"reading without reading\", where we rely on quantification and measurement to study large text collections.\n",
        "\n",
        "In this notebook, we delve into a few concrete examples that demonstrate how the affordances of LLMs might help with distant reading in novel ways.\n",
        "- Summarization and \"baby-RAG\"\n",
        "- Speed up annotation and/or information extraction via structured generation\n",
        "\n",
        "## Goals of this Session\n",
        "\n",
        "This notebook covers a few practical and theoretical aspects of working with LLMs in the context of humanities research. The goal is to start a discussion on:\n",
        " - Where to find and how to deploy open-source LLMs?\n",
        " - What tasks would make sense? Which models work well for a selected task?\n",
        " - How to evaluate outcomes and performance?\n",
        "\n",
        "We want to keep things simple!\n",
        "\n",
        "We will be playing with Llama-3 and get a feeling of how this changes the way we process and interrogate data.\n",
        "\n",
        "\n",
        "## Technical note\n",
        "\n",
        "We will be relying on the Hugging Face `InferenceClient` for accessing LLMs. These are freely accessible, but rate limits apply! If you would want to deploy a 'local' version (we're still on Colab, but the code should also work on your computer), uncomment the code below (where indicated) and make sure you are using a [GPU](https://cloud.google.com/gpu). To select a GPU on Colab Go to **`Runtime`** and select **`Change runtime type`**, then select `T4 GPU` (or any other GPU available).\n",
        "\n",
        "\n",
        "\n",
        "This notebook is inspired by: https://huggingface.co/learn/cookbook/structured_generation"
      ],
      "metadata": {
        "id": "E_e5BL9Ti3JG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install the transformer and other libraries\n",
        "!pip install -q -U \"transformers==4.40.0\" pydantic accelerate outlines datasets"
      ],
      "metadata": {
        "id": "qxa9ES0zX9ic"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Hugging Face Hub\n",
        "\n",
        "In the examples below, we will experiment with `Llama-3-8B-Instruct`, a recent series of open-source LLMs created by Meta. To use Llama3 you need to:\n",
        "\n",
        "- Make an account on Hugging Face https://huggingface.co/\n",
        "- Go to the Llama-3-8B and sign the terms of use you should get a reply swiftly https://huggingface.co/meta-llama/Meta-Llama-3-8B\n",
        "- Create a user access token with at least read access: https://huggingface.co/docs/hub/en/security-tokens\n",
        "- Run the code cell below to log into the Hugging Face hub. Copy-paste the access token.\n",
        "- Reply `n` to the question 'Add token as git credential? (Y/n)'"
      ],
      "metadata": {
        "id": "CXUryNZSXUd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIuiPmuRV9aU",
        "outputId": "b66c8ad7-78fd-4e50-a59a-2e95e052dc41"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing model and data"
      ],
      "metadata": {
        "id": "gpNX4yD1ch09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries"
      ],
      "metadata": {
        "id": "RtfaU-z2cLRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore') # disable warnings"
      ],
      "metadata": {
        "id": "IivyFPPNQ0t-"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from huggingface_hub import InferenceClient\n",
        "from datasets import Dataset\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import torch\n",
        "import pandas as pd\n",
        "import json\n",
        "pd.set_option(\"display.max_colwidth\", 100)"
      ],
      "metadata": {
        "id": "27FpdOCLasrJ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load model"
      ],
      "metadata": {
        "id": "Xtl7poXAcMdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# choose a LLMs model\n",
        "repo_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "# instantiate the inference client\n",
        "llm_client = InferenceClient(model=repo_id, timeout=120)"
      ],
      "metadata": {
        "id": "6RiZovNJXdlf"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # define the model, we use the instruct variant\n",
        "# checkpoint = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "# device = 'cuda' # make sure you use a GPU\n",
        "\n",
        "# # instantiate a text generation pipeline\n",
        "# pipeline = transformers.pipeline(\n",
        "#     \"text-generation\",\n",
        "#     model=checkpoint,\n",
        "#     model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
        "#     device=\"cuda\",\n",
        "# )\n",
        "\n",
        "# # some fluff to improve the generation\n",
        "# terminators = [\n",
        "#     pipeline.tokenizer.eos_token_id,\n",
        "#     pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "# ]"
      ],
      "metadata": {
        "id": "Uc9ROzqBjxan"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download data\n",
        "\n",
        "We will be experimenting with a small set of 10k British newspaper articles provided by the [\"Heritage Made Digital\"](https://blogs.bl.uk/thenewsroom/2019/01/heritage-made-digital-the-newspapers.html) project. Data was kindly prepared and provided by my colleague [Nilo Pedrazzini](https://www.linkedin.com/in/nilopedrazzini)"
      ],
      "metadata": {
        "id": "lk732HeUcOvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download a sample of 10.000 newspaper articles\n",
        "!wget -q --show-progress https://github.com/kasparvonbeelen/lancaster-newspaper-workshop/raw/wc/data/sample_lwm_hmd_mt90_10000.csv.zip\n",
        "# unzip the downloaded sample\n",
        "!unzip -o sample_lwm_hmd_mt90_10000.csv.zip\n",
        "!rm -r __MACOSX"
      ],
      "metadata": {
        "id": "bj-797e0Z9zr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('sample_lwm_hmd_mt90_10000.csv')\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "CP6EV9WhZXUR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "tE6fnaaRZgzU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process data\n",
        "\n",
        "To facilitate the analysis we divide the newspaper articles into smaller chunks of 250 words (with a 50-word overlap)."
      ],
      "metadata": {
        "id": "JYvK2zSTcUg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_chunks(text: str, size: int=250,step: int=50) -> list:\n",
        "  \"\"\"divide a text into chunks of similar size\n",
        "  Arguments:\n",
        "    text (str): input text\n",
        "    size (int): number of tokens in each chunk\n",
        "    step (int): step size\n",
        "  Returns a list of strings\n",
        "  \"\"\"\n",
        "  words = text.split()\n",
        "  return [' '.join(words[i:i+size]) for i in range(0,len(words),step)]"
      ],
      "metadata": {
        "id": "fE6QOcPqaf1I"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We save the chunks in a new list."
      ],
      "metadata": {
        "id": "FeSqqV6Wa2Ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply chunking to text\n",
        "df['chunks'] = df.text.apply(get_chunks)"
      ],
      "metadata": {
        "id": "Px_o2GN_Zn8K"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df.text[0]),len(df['chunks'][0])"
      ],
      "metadata": {
        "id": "NI7zi3fqZ-3o"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we reorder the dataframe: each chunk of 250 will be a new row (this increases the number of rows quite a bit, as you may observe)."
      ],
      "metadata": {
        "id": "5kVFcYiK2j2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reorder the dataframe\n",
        "# with one chunk in each row\n",
        "# instead of the whole text\n",
        "df_chunks = df.explode('chunks')\n",
        "df_chunks.shape"
      ],
      "metadata": {
        "id": "vYGjL1nPZ4ZV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompting\n",
        "\n",
        "LLM generate text from an input, usually referred to as a 'prompt', a piece of text we like the model to use as a starting point for predicting novel tokens.\n",
        "\n",
        "When 'chatting' with an LLM we usually provide the model with (at least) two messages: a system and a user prompt or message.\n",
        "\n",
        "**System message**:\n",
        "\n",
        "- **Generic instructions on behaviour**: specify how the model should behave (e.g. be helpful, respectful, neutral) or the role it should play (e.g., a teacher, assistant, or advisor).\n",
        "- **Constraints**: Specific instructions on what the model should avoid or how it should generate responses.\n",
        "- **Context**: Background information or context that remains constant throughout the session to ensure consistency.\n",
        "\n",
        "**User message**:\n",
        "\n",
        "- **Query**: specifies input from the user, such as a question, instruction, or request that the model needs to respond to.\n",
        "- **Dynamic**: changes with each interaction, reflecting the user's immediate needs, questions, or instructions.\n",
        "\n",
        "The Hugging Face chat prompt template allows messages as lists of dictionaries.\n",
        "\n",
        "```python\n",
        "messages [\n",
        " {\n",
        "    \"role\" : \"system\",\n",
        "    \"content\": \"<system prompt here>\"\n",
        " },\n",
        " {\n",
        "    \"role\" : \"user\",\n",
        "    \"content\": \"<user prompt here>\"\n",
        " }\n",
        "]\n",
        "```"
      ],
      "metadata": {
        "id": "QY1ucJhFnguw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a message by articulating a system and user prompt."
      ],
      "metadata": {
        "id": "GNYCb3IqoRZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"\n",
        "          You are a helpful AI that will assist me with analysing and reading newspaper articles.\n",
        "          Read the newspaper article attentively and extract the required information.\n",
        "          Each newspaper article is enclosed with triple hashtags (i.e. ###).\n",
        "          Don't make things up! If the information is not in the article then reply 'I don't know'\n",
        "          \"\"\"\n",
        "              },\n",
        "\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"Provide a short description of principal characters portrayed in the newspaper article?\n",
        "\n",
        "                  ###POOR T,i,ENIPAT A 1„k CT  The Poor Law Coirdnissioti(rs have issued a ei; cular,\n",
        "                  dated the 20th instant, stating that they have consulted the Attorney and\n",
        "                  Solicitor-General on the construction of the late Removal Act, and give as the\n",
        "                  result:— I. \" That the proviso to the Ist section of the 9 and 10 Vict., c. 66,\n",
        "                  which sets forth the exceptions to the principal enactments that are to be\n",
        "                  excluded in the computation of time, is net retrospective in its operation, so\n",
        "                  as to apply to cases where the five years\\' residence was complete before the statute.\n",
        "                  2. \" That an interval between the completion of the five years residence and the\n",
        "                  application for the warrant of removal filled up by one of the exceptions contained\n",
        "                  in the proviso will not p event the operation of the statute in restraining the\n",
        "                  removal of the pauper whu had resided for the specified time. 3. \" That orders\n",
        "                  of removal obtained previous to th• passing of the Act, but not then executed\n",
        "                  by the removal of the paupers,###\"\"\"\n",
        "              }\n",
        "  ]"
      ],
      "metadata": {
        "id": "AO5PEGlsUukK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages"
      ],
      "metadata": {
        "id": "zh2iS37UBIOU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#help(llm_client.chat_completion)"
      ],
      "metadata": {
        "id": "GyBwFdkBZ6RG"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import truncate\n",
        "# # uncomment this code if you want to work locally, comment the other function\n",
        "# def get_completion(messages: list, temperature=.1, top_p=.1) -> str:\n",
        "#   \"\"\"get completion for given system and user prompt\n",
        "#     Arguments:\n",
        "#     messages (list): a list containin a system and user message as\n",
        "#       python dictionaries with keys 'role' and 'content'\n",
        "#     temperature (float): regulate creativity of the text generation\n",
        "#     top_p (float): cummulative probability included in the\n",
        "#       generation process\n",
        "#   \"\"\"\n",
        "#   prompt = pipeline.tokenizer.apply_chat_template(\n",
        "#         messages,\n",
        "#         tokenize=False,\n",
        "#         add_generation_prompt=True\n",
        "#       )\n",
        "\n",
        "#   outputs = pipeline(\n",
        "#     prompt,\n",
        "#     max_new_tokens=256,\n",
        "#     eos_token_id=terminators,\n",
        "#     do_sample=True,\n",
        "#     temperature=temperature,\n",
        "#     top_p=top_p,\n",
        "#       )\n",
        "#   return outputs[0][\"generated_text\"][len(prompt):]\n",
        "\n",
        "\n",
        "def get_completion(messages: list, temperature=.1, top_p=.1):\n",
        "    \"\"\"get completion for given system and user prompt\n",
        "      Arguments:\n",
        "        messages (list): a list containin a system and user message as\n",
        "          python dictionaries with keys 'role' and 'content'\n",
        "        temperature (float): regulate creativity of the text generation\n",
        "        top_p (float): cummulative probability included in the\n",
        "          generation process\n",
        "    \"\"\"\n",
        "    outputs = llm_client.chat_completion(\n",
        "        messages=messages,\n",
        "        max_tokens=1024,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p\n",
        "        )\n",
        "    return outputs.choices[0].message.content"
      ],
      "metadata": {
        "id": "-MM2Wlv_Vw3y"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_completion(messages))"
      ],
      "metadata": {
        "id": "f6G6Cbt18lUX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise\n",
        "\n",
        "- Change the system message and ask the model to reply in medieval French.\n",
        "- Change the user message and ask the model to summarize the article and condense it to one sentence."
      ],
      "metadata": {
        "id": "SML4OsbfXIk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter code here"
      ],
      "metadata": {
        "id": "0CfZ-Q96omxn"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solution"
      ],
      "metadata": {
        "id": "Tq0pWVJXoo10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"\"\"\n",
        "    You are a helpful AI that will assist me with analysing and reading newspaper articles.\n",
        "    Read the newspaper article attentively and extract the required information.\n",
        "    Each newspaper article is enclosed with triple hashtags (i.e. ###).\n",
        "    Don't make things up! If the information is not in the article then reply 'I don't know'\n",
        "          Answer in medieval French!\"\"\"\n",
        "          },\n",
        "    {\"role\": \"user\", \"content\": f\"\"\"Provide a short description of principal characters portrayed newspaper article?\n",
        "    ###{df.iloc[0].text}###\"\"\"}\n",
        "]\n",
        "\n",
        "print(get_completion(messages))\n"
      ],
      "metadata": {
        "id": "am1Ge38tXGP-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying text generation to historical documents\n"
      ],
      "metadata": {
        "id": "gNcYq_a-beX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1: Summarize Summaries\n",
        "\n",
        "Let's imagine we'd wish to know what happened in January 1899 but won't have time to read all the newspaper issues. Luckily, LLMs excel at summarization!"
      ],
      "metadata": {
        "id": "USeWpL8Ar-UX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We select all the articles for this January 1899 and save them in a new dataframe. For the purposes of this exercise, we just take a random sample of 20 chunks, otherwise it will take too long to run everything through the model."
      ],
      "metadata": {
        "id": "Fy5lpuy94eBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_small = df_chunks[\n",
        "            (df_chunks.year==1899) & (df_chunks.month==1) # select articles from January 1899\n",
        "                  ].sample(20, random_state=1984).reset_index(drop=True) # we sample a few to keep things simple\n",
        "df_small.shape"
      ],
      "metadata": {
        "id": "Cw-N3XoUkRfu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fefd790-1128-4961-ee4e-866e8abd647f"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the cell below to load the `apply_completions` function."
      ],
      "metadata": {
        "id": "CIm6yKDE41Lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_completions(item: pd.Series,\n",
        "                      system_message: str,\n",
        "                      user_message: str,\n",
        "                      text_column: str = 'chunks') -> str:\n",
        "  \"\"\"\n",
        "  Function that appl\n",
        "  Argument:\n",
        "    item (pd.Series): row from a pandas Dataframe\n",
        "    system_message (str): system prompt, specifies how the system\n",
        "      should behave in\n",
        "    user_message (str): user prompt, give instruction how to\n",
        "      process each historical. the documents itself will be append\n",
        "      from the 'text_column' argument\n",
        "    text_column (str): name of the text column\n",
        "  \"\"\"\n",
        "  messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_message}\n",
        "      ]\n",
        "  messages[1]['content'] += f\"\\n\\n###{item[text_column]}###\"\n",
        "  return  get_completion(messages)"
      ],
      "metadata": {
        "id": "FJiND9t_dkE0"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We apply the prompt to the text chunks in our dataframe."
      ],
      "metadata": {
        "id": "OwuTEn3245AK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas() # use tqdm to view progress\n",
        "\n",
        "system_message = \"\"\"\n",
        "    You are a helpful AI that will assist me with analysing and reading newspaper articles.\n",
        "    Read the newspaper article attentively and extract the required information.\n",
        "    Each newspaper article is enclosed with triple hashtags (i.e. ###).\n",
        "    Don't make things up! If the information is not in the article then reply 'I don't know'\n",
        "    \"\"\"\n",
        "user_message = \"Summarize the article in one sentence.\"\n",
        "\n",
        "df_small['completion'] =  df_small.progress_apply(apply_completions,system_message=system_message, user_message=user_message, axis=1)"
      ],
      "metadata": {
        "id": "nUEjoux9Y3ZV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print the summaries\n",
        "df_small['completion'][0]"
      ],
      "metadata": {
        "id": "n54N-ySxjuN2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, we can condense information even more by summarizing the summaries!"
      ],
      "metadata": {
        "id": "1tNVVaLH5TN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new string from the summaries with each between triple hashtags\n",
        "summaries = '\\n'.join([f\"###{c}###\" for c in df_small['completion']])"
      ],
      "metadata": {
        "id": "OyeisiEc9NFV"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new user message\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": f\"\"\"Based on the article summaries within ### below, what are the most important events? Be concise\\n{summaries}\"\"\"}\n",
        "      ]"
      ],
      "metadata": {
        "id": "HhMcFo_J8Y5s"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_completion(messages))"
      ],
      "metadata": {
        "id": "kTtQH8El8XWh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Condense information about accidents\n",
        "\n",
        "How did accidents in the news change over time? In this example, we analyse accident reports using a simple pipeline using summarisation and baby-RAG (in the sense that we first retrieve and then generate a response to our query).\n",
        "\n",
        "\n",
        "In the first step we simple use a regular expression to find reports about accidents."
      ],
      "metadata": {
        "id": "JzAyo9DJKhuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = re.compile(r'\\baccidents?\\b', re.I) # compile a regex\n",
        "pattern.findall('accidents accident AccIdent accidental') # test the regex on a few example"
      ],
      "metadata": {
        "id": "TDv5HAz1LpW4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas()\n",
        "df_chunks['matches'] = df_chunks.chunks.progress_apply(lambda x: bool(pattern.findall(x)))"
      ],
      "metadata": {
        "id": "64Q74D2MMISw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we retrieve a small sample of accident reports by decade (for the 1810s and 189s)."
      ],
      "metadata": {
        "id": "KpUwL6O3O3cw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accident_1810s = df_chunks[\n",
        "                    (df_chunks.year.between(1810,1820)) & (df_chunks['matches'] == True)\n",
        "                      ].sample(n=10, random_state=1984)\n",
        "\n",
        "accident_1890s = df_chunks[\n",
        "                    (df_chunks.year.between(1890,1900)) & (df_chunks['matches'] == True)\n",
        "                      ].sample(n=10, random_state=1984)\n",
        "print(accident_1810s.shape,accident_1890s.shape)"
      ],
      "metadata": {
        "id": "3j0Vs5IxMiUO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use `.value_counts()` to compute the total number of articles mentioning 'accident' at least once."
      ],
      "metadata": {
        "id": "WGHaHfoAPGTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(df_chunks['matches'] == True).value_counts()"
      ],
      "metadata": {
        "id": "tBvyenk2W7QC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accident_1890s.iloc[3].chunks"
      ],
      "metadata": {
        "id": "-cuWnOlpVVZy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"\"\"\n",
        "    You are a helpful AI that will assist me with analysing and reading newspaper articles.\n",
        "    Read the newspaper article attentively and extract the required information.\n",
        "    Each newspaper article is enclosed with triple hashtags (i.e. ###).\n",
        "    Don't make things up! If the information is not in the article then reply 'I don't know'\n",
        "    Focus on the answer and do not add any unnecessary texts.\"\"\"\n",
        "user_message = \"\"\"Does the article talk about an accident?\n",
        "If yes summarize the article content in one sentence.\n",
        "If not, answer 'No accident mentioned' \"\"\"\n",
        "\n",
        "accident_1810s['completion'] =  accident_1810s.progress_apply(apply_completions,system_message=system_message, user_message=user_message, axis=1)\n",
        "accident_1890s['completion'] =  accident_1890s.progress_apply(apply_completions,system_message=system_message, user_message=user_message, axis=1)"
      ],
      "metadata": {
        "id": "1FtRSk2oT5sc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accident_1890s[['chunks','completion']].iloc[3].values"
      ],
      "metadata": {
        "id": "c46YdAkqU9_K"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we can group the summaries by decade and ask the LLM to figure out the principal differences and similarities. Instead of quantifying, the increasingly condense information as a method for distant reading.\n",
        "\n",
        "Obviously, making more targeted prompts could help, for example, we could ask the LLMs to focus on the machines in the summary or the gender of victims."
      ],
      "metadata": {
        "id": "bOwTEry7Pmo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summaries_1810s = '\\n'.join([f\"###{c}###\" for c in accident_1810s.chunks if not c.lower().startswith('no')])\n",
        "summaries_1810s = f\"\\n```\\nSummaries for 1810s:\\n\\n{summaries_1810s}\\n```\"\n",
        "summaries_1890s = '\\n'.join([f\"###{c}###\" for c in accident_1890s.chunks if not c.lower().startswith('no')])\n",
        "summaries_1890s = f\"\\n```\\nSummaries for 1810s:\\n\\n{summaries_1890s}\\n```\"\n",
        "print(summaries_1890s)"
      ],
      "metadata": {
        "id": "hf8DODZwcqvD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": f\"\"\"\n",
        "        Below we provide articles from two different decades. First from 1810s and then from the 1890s.\n",
        "        Each of the decades is enclosed within ```.\n",
        "        Each summary is enclosed withing ###\n",
        "        Answer the following question concisely: what the principal differences between the two decades?\n",
        "        \\n{summaries_1810s+summaries_1890s}\"\"\"}\n",
        "      ]\n",
        "print(get_completion(messages))"
      ],
      "metadata": {
        "id": "RfMEPcuDUadY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3: Structured Generation\n",
        "\n",
        "Newspapers contain a lot of biographical information, one could say biography appears as a microgenre in the press. For example, in accident reports we do get some background about the people involved, implicitly (gender) or explicitly (professions or age).\n",
        "\n",
        "Below we use a language model to extract such information from newspaper reports and return it in a predefined format that allows us to analyse newspapers as structured data.\n",
        "\n",
        "Put differently, we use LLMs to extract information similar to automatic annotation, and convert text to tabular format."
      ],
      "metadata": {
        "id": "ov1ySBNokZmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_small = df_chunks[\n",
        "                    (df_chunks['matches'] == True)\n",
        "                      ].sample(n=10, random_state=1984)"
      ],
      "metadata": {
        "id": "wKWWeBVXoFaG"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_small['chunks'].iloc[7]"
      ],
      "metadata": {
        "id": "wCE10sOIZSzt"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We rewrite the system prompt and give it a few more instructions on how to respond to our queries."
      ],
      "metadata": {
        "id": "cDSoGMluRrVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"\"\"You are an helpful AI that will assist me with analysing source documents in the form of historical newspaper articles.\n",
        "    Read the newspaper articles attentively and extract structured information formatted as a list of Python dictionaries.\n",
        "    Provide all relevant short source snippets from the documents on which you directly based your answer.\n",
        "    Keep the source snippet short to just a few words and not complete sentences.\n",
        "    The snippet MUST be extracted from the soutce, with spelling and wording identical to the source.\n",
        "    This list of JSON blobs should begin with a \"START\" tag and end with a \"END\" tag.\n",
        "    Each newspaper article will be enclosed with triple hash tags (i.e. ###).\n",
        "    Don't make thigs up! If you don't know the answer, simply return no value\"\"\"\n",
        "\n",
        "\n",
        "user_message = \"\"\"\n",
        "If the article describes a historical accident, extract biographical information about the individuals involved in the accidents.\n",
        "Return a list of Python dictionaries for each individual which records important personal attributes such gender, age and profession, and others that are relevant.\n",
        "Each attribute is a key in a dictionary.\n",
        "Record personal attribures as dictionaries as shown in the example below.\n",
        "Also add one key with \"outcome\" that records what happened to person (\"drowned\", \"survived\", \"injured\")\n",
        "Add a confidence score as a float between 0 and 1 for each snippet extracted.\n",
        "Under \"source_snippets\" collect text fragments that record what happened to person involved.\n",
        "\n",
        "START\n",
        "[\n",
        "  {\n",
        "  \"name\" : { \"value\": answer,\"source\": source_snippet, \"confidence\": your_confidence_score },\n",
        "  \"gender\" : { \"value\": answer,\"source\": source_snippet, \"confidence\": your_confidence_score },\n",
        "  \"profession\" :{ \"value\": answer,\"source\": source_snippet, \"confidence\": your_confidence_score },\n",
        "  ... other attributes ...,\n",
        "  \"outcome\" : { \"value\": answer,\"source\": source_snippet, \"confidence\": your_confidence_score },\n",
        "  \"summary\": { \"value\" :summary, \"confidence\" : your_confidence_score }\n",
        "  },\n",
        "...]\n",
        "END\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "xAYqIGPOheVr"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_message + f'\\n\\n###{df_small[\"chunks\"].iloc[4]}###'}\n",
        "      ]\n",
        "print(get_completion(messages))"
      ],
      "metadata": {
        "id": "3S9M6ojCd0d7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_small['completion'] =  df_small.progress_apply(apply_completions,system_message=system_message, user_message=user_message, axis=1)\n"
      ],
      "metadata": {
        "id": "cDw5FzAta9Ku"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_small['completion'])"
      ],
      "metadata": {
        "id": "BJGmFs9Qq_Tu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To convert the response to a Python data type, we use the `eval_completion` function."
      ],
      "metadata": {
        "id": "ocO1ody2SAWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_completion(completion: str) -> list:\n",
        "  \"\"\"Convert the completion as string to a Python list\n",
        "  Argument:\n",
        "      completion (str): structured generation by LLM\n",
        "  \"\"\"\n",
        "  try:\n",
        "    return eval(completion.split('START')[-1].strip().rstrip('END').strip())\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    return []\n",
        "\n",
        "df_small['completion_eval'] = df_small['completion'].apply(eval_completion)"
      ],
      "metadata": {
        "id": "NmyQ5u4NIIT_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a bit closer look at some examples."
      ],
      "metadata": {
        "id": "CBwc1JXhSgK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_small['completion_eval']"
      ],
      "metadata": {
        "id": "0boRbvyXIsdW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_small['completion_eval'].iloc[7]"
      ],
      "metadata": {
        "id": "tS6AD3QbpL0t"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly we can have a bit closer look at how the language model processes the text by highlighting the fragments on which it based its answers. This can help us with\n",
        "- creating automatic pre-annotation\n",
        "- figuring out how the pipeline could be improved\n",
        "- close-reading large amounts of text"
      ],
      "metadata": {
        "id": "WXbqMNySJ2ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row = df_small.iloc[7]\n",
        "html_output = row['chunks']\n",
        "for p_dict in row['completion_eval']:\n",
        "  for attr, attr_dict in p_dict.items():\n",
        "    try:\n",
        "      if isinstance(attr_dict, dict):\n",
        "        if attr_dict.get('confidence',.0) > .5 and attr_dict.get(\"source\",None):\n",
        "          html_output = re.sub(str(attr_dict['source']),\n",
        "                   f'<span style=\"background-color: yellow;\">{attr_dict[\"source\"]}</span>', html_output)\n",
        "    except Exception as e:\n",
        "      print(e,attr_dict)\n",
        "      continue"
      ],
      "metadata": {
        "id": "NZD1DtdgJqPG"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import HTML\n",
        "HTML(html_output)"
      ],
      "metadata": {
        "id": "41udlbVTLeLr"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: OCR correction"
      ],
      "metadata": {
        "id": "CWAog9xdpG_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, let's use LLM to help us with a longstanding problem in digital humanities, improving OCR quality."
      ],
      "metadata": {
        "id": "jmvJn6ZlOdC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_small_bad_ocr = df_chunks.sort_values('ocrquality', ascending=True)[:1000].sample(n=10)"
      ],
      "metadata": {
        "id": "G5N8e2qdpKBZ"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"You are an helpful AI and provide truthful correction of historical text.\"\n",
        "\n",
        "user_message = \"\"\"Transcribe the text and correct typos and errors in the text caused by bad optical character recognition (OCR).\n",
        "Do not add any information that is not in the original text!\"\"\"\n",
        "\n",
        "df_small_bad_ocr['completion'] = df_small_bad_ocr.progress_apply(apply_completions,system_message=system_message, user_message=user_message, axis=1)\n"
      ],
      "metadata": {
        "id": "4JXH_oypolYi"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_small_bad_ocr.iloc[0]['chunks']"
      ],
      "metadata": {
        "id": "wjFbCSDYtNx-"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_small_bad_ocr.iloc[0]['completion'])"
      ],
      "metadata": {
        "id": "XLsWShc7tKGs"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_small_bad_ocr.iloc[3]['chunks']"
      ],
      "metadata": {
        "id": "6rc0U8WqsXiA"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_small_bad_ocr.iloc[3]['completion']"
      ],
      "metadata": {
        "id": "Y5BxeDnttYZ7"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_small_bad_ocr.to_csv('newspaper_ocr_corrected.csv')"
      ],
      "metadata": {
        "id": "BpChnh_7q-gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise\n",
        "\n",
        "Experiment with your own system and user message! Have fun :-)"
      ],
      "metadata": {
        "id": "FzavYR10sI_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# enter code here"
      ],
      "metadata": {
        "id": "OyrtAOQ24cJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fin."
      ],
      "metadata": {
        "id": "XzdNxsZPsKr5"
      }
    }
  ]
}